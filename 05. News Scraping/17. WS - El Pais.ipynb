{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping: El Pais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain list of news from the coverpage\n",
    "\n",
    "URL definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url definition\n",
    "url = \"https://elpais.com/elpais/inenglish.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Request\n",
    "r1 = requests.get(url)\n",
    "r1.status_code\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "coverpage = r1.content\n",
    "\n",
    "# Soup creation\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "# News identification\n",
    "coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "len(coverpage_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list in which every element is a news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"articulo-titulo\" itemprop=\"headline\">\n",
       "<a href=\"https://elpais.com/elpais/2019/12/26/inenglish/1577353137_122414.html\">Politicians from all sides of the spectrum respond to King Felipeâ€™s Christmas speech</a>\n",
       "</h2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's extract the text from the articles:\n",
    "\n",
    "First, we'll define the number of articles we want: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    # only news articles (there are also albums and other things)\n",
    "    if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "        continue\n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them into:\n",
    "\n",
    "- a dataset which will the input of the models (df_features)\n",
    "- a dataset with the title and the link (df_show_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features\n",
    "df_features = pd.DataFrame(\n",
    "     {'Article Content': news_contents \n",
    "    })\n",
    "\n",
    "# df_show_info\n",
    "df_show_info = pd.DataFrame(\n",
    "    {'Article Title': list_titles,\n",
    "     'Article Link': list_links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>At least three suspected members of the Main I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Drowning was the sole cause of death for the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ilyas El Masdouri knows what it feels like to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Former Catalan premier, Carles Puigdemont, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Spanish politicians from all sides of the poli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Article Content\n",
       "0  At least three suspected members of the Main I...\n",
       "1  Drowning was the sole cause of death for the t...\n",
       "2  Ilyas El Masdouri knows what it feels like to ...\n",
       "3  Former Catalan premier, Carles Puigdemont, has...\n",
       "4  Spanish politicians from all sides of the poli..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Article Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Three suspected Russian spies traveled to Barc...</td>\n",
       "      <td>https://elpais.com/elpais/2019/12/27/inenglish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>British victims who drowned in Costa del Sol r...</td>\n",
       "      <td>https://elpais.com/elpais/2019/12/27/inenglish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Meet the migrants helping the homeless in the ...</td>\n",
       "      <td>https://elpais.com/elpais/2019/12/26/inenglish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ousted Catalan premier calls for European arre...</td>\n",
       "      <td>https://elpais.com/elpais/2019/12/27/inenglish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Politicians from all sides of the spectrum res...</td>\n",
       "      <td>https://elpais.com/elpais/2019/12/26/inenglish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Article Title  \\\n",
       "0  Three suspected Russian spies traveled to Barc...   \n",
       "1  British victims who drowned in Costa del Sol r...   \n",
       "2  Meet the migrants helping the homeless in the ...   \n",
       "3  Ousted Catalan premier calls for European arre...   \n",
       "4  Politicians from all sides of the spectrum res...   \n",
       "\n",
       "                                        Article Link  \n",
       "0  https://elpais.com/elpais/2019/12/27/inenglish...  \n",
       "1  https://elpais.com/elpais/2019/12/27/inenglish...  \n",
       "2  https://elpais.com/elpais/2019/12/26/inenglish...  \n",
       "3  https://elpais.com/elpais/2019/12/27/inenglish...  \n",
       "4  https://elpais.com/elpais/2019/12/26/inenglish...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Elapsed\n",
    "\n",
    "We are interested in how much time does the script takes to get the news because this will impact directly on user experience. For this, we'll put it all into a single function and then call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_elpais():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://elpais.com/elpais/inenglish.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "    \n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # only news articles (there are also albums and other things)\n",
    "        if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "        \n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'El Pais English'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapsed is 6.523742 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x, y = get_news_elpais()\n",
    "end =time.time()\n",
    "te = end-start\n",
    "print(\"The time elapsed is %f seconds\" %(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a fine number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
