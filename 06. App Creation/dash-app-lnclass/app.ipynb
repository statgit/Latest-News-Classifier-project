{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:251: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator SVC from version 0.19.1 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:251: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:251: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:15] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:15] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "[2020-10-15 17:13:15,567] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 967, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 907, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 625, in update_table\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 408, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\common.py\", line 218, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:15] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2020-10-15 17:13:15,570] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 967, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 907, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 582, in update_piechart\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 408, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\common.py\", line 218, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:15] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2020-10-15 17:13:15,572] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 967, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 907, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 509, in update_barchart\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 408, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\common.py\", line 218, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:15] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2020-10-15 17:13:31,087] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 967, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 907, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 489, in scrape_and_predict\n",
      "    df_features = df_features.append(get_news_themirror()[0])\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 222, in get_news_themirror\n",
      "    link = coverpage_news[n]['href']\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Oct/2020 17:13:31] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2020-10-15 17:13:39,718] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 967, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dash\\dash.py\", line 907, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 489, in scrape_and_predict\n",
      "    df_features = df_features.append(get_news_themirror()[0])\n",
      "  File \"<ipython-input-1-dbf92a0afe7f>\", line 222, in get_news_themirror\n",
      "    link = coverpage_news[n]['href']\n",
      "IndexError: list index out of range\n",
      "127.0.0.1 - - [15/Oct/2020 17:13:39] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "from dash.dependencies import Input, Output, State, Event\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "\n",
    "# Importing the  inputs\n",
    "\n",
    "path_models = \"Pickles/\"\n",
    "# SVM\n",
    "path_svm = path_models + 'best_svc.pickle'\n",
    "with open(path_svm, 'rb') as data:\n",
    "    svc_model = pickle.load(data)\n",
    "\n",
    "path_tfidf = \"Pickles/tfidf.pickle\"\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)\n",
    "\n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4,\n",
    "    'other':5\n",
    "}\n",
    "\n",
    "# Definition of functions\n",
    "# El Pais\n",
    "def get_news_elpais():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://elpais.com/elpais/inenglish.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # only news articles (there are also albums and other things)\n",
    "        if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html.parser')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'El Pais English'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "# The Guardian\n",
    "def get_news_theguardian():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.theguardian.com/uk\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h3', class_='fc-item__title')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 8\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # We need to ignore \"live\" pages since they are not articles\n",
    "        if \"live\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "            \n",
    "        # We also need to ignore \"commentisfree\" pages\n",
    "        if \"commentisfree\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "            \n",
    "        # And \"ng-interactive\" pages\n",
    "        if \"ng-interactive\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html.parser')\n",
    "        body = soup_article.find_all('div', class_='content__article-body from-content-api js-article__body')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'The Guardian'})\n",
    "\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "\n",
    "# The Mirror\n",
    "def get_news_themirror():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.mirror.co.uk/\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('a', class_='headline publication-font')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n]['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html.parser')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = soup_article.find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'The Mirror'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category\n",
    "\n",
    "def predict_from_features(features):\n",
    "        \n",
    "    # Obtain the highest probability of the predictions for each article\n",
    "    predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions_pre = svc_model.predict(features)\n",
    "\n",
    "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
    "    predictions = []\n",
    "\n",
    "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
    "        if prob > .65:\n",
    "            predictions.append(cat)\n",
    "        else:\n",
    "            predictions.append(5)\n",
    "\n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df\n",
    "\n",
    "# Dash App\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "server = app.server\n",
    "\n",
    "#####\n",
    "# Edit from here\n",
    "#####\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'background': '#ffffff',\n",
    "    'text': '#696969',\n",
    "    'header_table': '#ffedb3'\n",
    "}\n",
    "\n",
    "# Markdown text\n",
    "markdown_text1 = '''\n",
    "This application gathers the latest news from the newspapers **El Pais**, **The Guardian** and **The Mirror**, predicts their category between **Politics**, **Business**, **Entertainment**, **Sport**, **Tech** and **Other** and then shows a graphic summary.\n",
    "The news categories are predicted with a Support Vector Machine model.\n",
    "Please enter which newspapers would you like to scrape news off and press **Submit**:\n",
    "'''\n",
    "\n",
    "markdown_text2 = '''\n",
    "*The scraped news are converted into a numeric feature vector with TF-IDF vectorization. Then, a Support Vector Classifier is applied to predict each category.*\n",
    "Warning: The Mirror takes approximately 30 seconds to gather the news articles.\n",
    "Created by Badal Kumar.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor':colors['background']}, children=[\n",
    "    \n",
    "    # Title\n",
    "    html.H1(children='News Classification App',\n",
    "            style={\n",
    "                'textAlign': 'left',\n",
    "                'color': colors['text'],\n",
    "                'padding': '20px',\n",
    "                'backgroundColor': colors['header_table']\n",
    "\n",
    "            },\n",
    "            className='banner',\n",
    "           ),\n",
    "\n",
    "    # Sub-title Left\n",
    "    html.Div([\n",
    "        dcc.Markdown(children=markdown_text1)],\n",
    "        style={'width': '49%', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Sub-title Right\n",
    "    html.Div([\n",
    "        dcc.Markdown(children=markdown_text2)],\n",
    "        style={'width': '49%', 'float': 'right', 'display': 'inline-block'}),\n",
    "\n",
    "    # Space between text and dropdown\n",
    "    html.H1(id='space', children=' '),\n",
    "\n",
    "    # Dropdown\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            options=[\n",
    "                {'label': 'El Pais English', 'value': 'EPE'},\n",
    "                {'label': 'The Guardian', 'value': 'THG'},\n",
    "                {'label': 'The Mirror', 'value': 'TMI'}\n",
    "            ],\n",
    "            value=['EPE', 'THG'],\n",
    "            multi=True,\n",
    "            id='checklist')],\n",
    "        style={'width': '40%', 'display': 'inline-block', 'float': 'left'}),\n",
    "        \n",
    "\n",
    "    # Button\n",
    "    html.Div([\n",
    "        html.Button('Submit', id='submit', type='submit')],\n",
    "        style={'float': 'center'}),\n",
    "    \n",
    "    # Output Block\n",
    "    html.Div(id='output-container-button', children=' '),\n",
    "    \n",
    "    # Graph1\n",
    "    html.Div([\n",
    "        dcc.Graph(id='graph1')],\n",
    "        style={'width': '49%', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Graph2\n",
    "    html.Div([\n",
    "        dcc.Graph(id='graph2')],\n",
    "        style={'width': '49%', 'float': 'right', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Table title\n",
    "    html.Div(id='table-title', children='You can see a summary of the news articles below:'),\n",
    "\n",
    "    # Space\n",
    "    html.H1(id='space2', children=' '),\n",
    "    \n",
    "    # Table\n",
    "    html.Div([\n",
    "        dash_table.DataTable(\n",
    "            id='table',\n",
    "            columns=[{\"name\": i, \"id\": i} for i in ['Article Title', 'Article Link', 'Newspaper', 'Prediction']],\n",
    "            style_data={'whiteSpace': 'normal'},\n",
    "            style_as_list_view=True,\n",
    "            style_cell={'padding': '5px', 'textAlign': 'left', 'backgroundColor': colors['background']},\n",
    "            style_header={\n",
    "                'backgroundColor': colors ['header_table'],\n",
    "                'fontWeight': 'bold'\n",
    "            },\n",
    "            style_table={\n",
    "                'maxHeight': '300',\n",
    "                'overflowY':'scroll'\n",
    "                \n",
    "            },\n",
    "            css=[{\n",
    "                'selector': '.dash-cell div.dash-cell-value',\n",
    "                'rule': 'display: inline; white-space: inherit; overflow: inherit; text-overflow: inherit;'\n",
    "            }]\n",
    "        )],\n",
    "        style={'width': '75%','float': 'left', 'position': 'relative', 'left': '12.5%', 'display': 'inline-block'}),    \n",
    "        \n",
    "    # Hidden div inside the app that stores the intermediate value\n",
    "    html.Div(id='intermediate-value', style={'display': 'none'})\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'children'),\n",
    "    [],\n",
    "    [State('checklist', 'value')],\n",
    "    [Event('submit', 'click')])\n",
    "def scrape_and_predict(values):\n",
    "    \n",
    "    df_features = pd.DataFrame()\n",
    "    df_show_info = pd.DataFrame()\n",
    "    \n",
    "    if 'EPE' in values:\n",
    "        # Get the scraped dataframes\n",
    "        df_features = df_features.append(get_news_elpais()[0])\n",
    "        df_show_info = df_show_info.append(get_news_elpais()[1])\n",
    "    \n",
    "    if 'THG' in values:\n",
    "        df_features = df_features.append(get_news_theguardian()[0])\n",
    "        df_show_info = df_show_info.append(get_news_theguardian()[1])\n",
    "        \n",
    "    if 'TMI' in values:\n",
    "        df_features = df_features.append(get_news_themirror()[0])\n",
    "        df_show_info = df_show_info.append(get_news_themirror()[1])\n",
    "\n",
    "    df_features = df_features.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Create features\n",
    "    features = create_features_from_df(df_features)\n",
    "    # Predict\n",
    "    predictions = predict_from_features(features)\n",
    "    # Put into dataset\n",
    "    df = complete_df(df_show_info, predictions)\n",
    "    # df.to_csv('Tableau Teaser/df_tableau.csv', sep='^')  # export to csv to work out an example in Tableau\n",
    "    \n",
    "    return df.to_json(date_format='iso', orient='split')\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph1', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_barchart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df.groupby(['Newspaper', 'Prediction']).count()['Article Title']\n",
    "\n",
    "    # Create x and y arrays for the bar plot for every newspaper\n",
    "    if 'El Pais English' in df_sum.index:\n",
    "    \n",
    "        df_sum_epe = df_sum['El Pais English']\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [[df_sum_epe['politics'] if 'politics' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['business'] if 'business' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['entertainment'] if 'entertainment' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['sport'] if 'sport' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['tech'] if 'tech' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['other'] if 'other' in df_sum_epe.index else 0][0]]   \n",
    "    else:\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [0,0,0,0,0,0]\n",
    "    \n",
    "    if 'The Guardian' in df_sum.index:\n",
    "        \n",
    "        df_sum_thg = df_sum['The Guardian']\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [[df_sum_thg['politics'] if 'politics' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['business'] if 'business' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['entertainment'] if 'entertainment' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['sport'] if 'sport' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['tech'] if 'tech' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['other'] if 'other' in df_sum_thg.index else 0][0]]   \n",
    "    else:\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [0,0,0,0,0,0]\n",
    "\n",
    "    if 'The Mirror' in df_sum.index:\n",
    "    \n",
    "        df_sum_tmi = df_sum['The Mirror']\n",
    "        x_tmi = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_tmi = [[df_sum_tmi['politics'] if 'politics' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['business'] if 'business' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['entertainment'] if 'entertainment' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['sport'] if 'sport' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['tech'] if 'tech' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['other'] if 'other' in df_sum_tmi.index else 0][0]]   \n",
    "\n",
    "    else:\n",
    "        x_tmi = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_tmi = [0,0,0,0,0,0]\n",
    "\n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'x': x_epe, 'y':y_epe, 'type': 'bar', 'name': 'El Pais'},\n",
    "            {'x': x_thg, 'y':y_thg, 'type': 'bar', 'name': 'The Guardian'},\n",
    "            {'x': x_tmi, 'y':y_tmi, 'type': 'bar', 'name': 'The Mirror'}\n",
    "        ],\n",
    "        'layout': {\n",
    "            'title': 'Number of news articles by newspaper',\n",
    "            'plot_bgcolor': colors['background'],\n",
    "            'paper_bgcolor': colors['background'],\n",
    "            'font': {\n",
    "                    'color': colors['text']\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    "\n",
    "    return figure\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph2', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_piechart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df['Prediction'].value_counts()\n",
    "\n",
    "    # Create x and y arrays for the bar plot\n",
    "    x = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "    y = [[df_sum['politics'] if 'politics' in df_sum.index else 0][0],\n",
    "         [df_sum['business'] if 'business' in df_sum.index else 0][0],\n",
    "         [df_sum['entertainment'] if 'entertainment' in df_sum.index else 0][0],\n",
    "         [df_sum['sport'] if 'sport' in df_sum.index else 0][0],\n",
    "         [df_sum['tech'] if 'tech' in df_sum.index else 0][0],\n",
    "         [df_sum['other'] if 'other' in df_sum.index else 0][0]]\n",
    "    \n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'values': y,\n",
    "             'labels': x, \n",
    "             'type': 'pie',\n",
    "             'hole': .4,\n",
    "             'name': '% of news articles'}\n",
    "        ],\n",
    "        \n",
    "        'layout': {\n",
    "            'title': '% of news articles',\n",
    "            'plot_bgcolor': colors['background'],\n",
    "            'paper_bgcolor': colors['background'],\n",
    "            'font': {\n",
    "                    'color': colors['text']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return figure\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    Output('table', 'data'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_table(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    data = df.to_dict('rows')\n",
    "    return data\n",
    "\n",
    "    \n",
    "    \n",
    "# Loading CSS\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/bWLwgP.css\"})\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/brPBPO.css\"})\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "# To here\n",
    "#####\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
